{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb1e125",
   "metadata": {},
   "source": [
    "\n",
    "The pipeline implemented in the notebook typically follows these steps:\n",
    "\n",
    "1. **Data loading**  \n",
    "   Load `train.csv` and `test.csv` into pandas DataFrames. SMILES strings are read from the `SMILES` column for each polymer.\n",
    "\n",
    "2. **Molecule parsing and descriptor computation**  \n",
    "   - Use `safe_mol_from_smiles` to safely parse SMILES into RDKit molecule objects.  \n",
    "   - Compute molecular descriptors with `compute_rdkit_descriptors`.  \n",
    "   - Generate Morgan fingerprints with `mol_to_morgan_fp_array`.  \n",
    "   - Combine descriptors and fingerprints into a numeric feature matrix using `build_feature_matrix`.\n",
    "\n",
    "3. **Data cleaning & preprocessing**  \n",
    "   - Handle infinite values and drop columns with too many missing values using `data_cleaning`.  \n",
    "   - Fill remaining missing values with `KNNImputer` and clip extreme values.  \n",
    "   - Scale features before modeling using `StandardScaler` when required.\n",
    "\n",
    "4. **Feature selection**  \n",
    "   - Remove low-variance features and highly correlated columns.  \n",
    "   - Perform model-based feature selection using `feature_selection_pipeline`, which trains a LightGBM on a target (Tg) and selects top features by importance.\n",
    "\n",
    "5. **Modeling per target**  \n",
    "   - Train a LightGBM regressor for each property using `train_lgb_per_target`.  \n",
    "   - Use K-Fold cross-validation to generate out-of-fold predictions.  \n",
    "   - Scale test data with the same scaler, and average predictions across folds.\n",
    "\n",
    "6. **Ensembling & postprocessing**  \n",
    "   - Combine predictions from multiple models or folds to reduce variance and improve robustness.\n",
    "\n",
    "7. **Submission generation**  \n",
    "   - Create `submission.csv` containing predicted values for all target properties, ready for Kaggle submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43b64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import lightgbm as lgb\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abab9ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87817</td>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374645</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106919</td>\n",
       "      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388772</td>\n",
       "      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519416</td>\n",
       "      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539187</td>\n",
       "      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             SMILES  Tg       FFV  \\\n",
       "0   87817                         *CC(*)c1ccccc1C(=O)OCCCCCC NaN  0.374645   \n",
       "1  106919  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5... NaN  0.370410   \n",
       "2  388772  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(... NaN  0.378860   \n",
       "3  519416  *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)... NaN  0.387324   \n",
       "4  539187  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N... NaN  0.355470   \n",
       "\n",
       "         Tc  Density  Rg  \n",
       "0  0.205667      NaN NaN  \n",
       "1       NaN      NaN NaN  \n",
       "2       NaN      NaN NaN  \n",
       "3       NaN      NaN NaN  \n",
       "4       NaN      NaN NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR = \"./neurips-open-polymer-prediction-2025\"\n",
    "train = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
    "test  = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557d637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tg         7462\n",
      "FFV         943\n",
      "Tc         7236\n",
      "Density    7360\n",
      "Rg         7359\n",
      "dtype: int64 (7973, 7)\n"
     ]
    }
   ],
   "source": [
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "# Na counts\n",
    "na_counts = train[targets].isna().sum()\n",
    "print(na_counts, train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6190114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:32.631240Z",
     "iopub.status.busy": "2025-12-04T17:02:32.630056Z",
     "iopub.status.idle": "2025-12-04T17:02:32.653978Z",
     "shell.execute_reply": "2025-12-04T17:02:32.651338Z"
    },
    "papermill": {
     "duration": 0.033504,
     "end_time": "2025-12-04T17:02:32.656343",
     "exception": false,
     "start_time": "2025-12-04T17:02:32.622839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_mol_from_smiles(smi):\n",
    "    \"\"\"Safely convert a SMILES string into an RDKit Mol object.\n",
    "    Returns None if parsing fails.\"\"\"\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        return m\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_rdkit_descriptors(df_smiles, desc_names=None):\n",
    "    \"\"\"Compute RDKit molecular descriptors for a Series of SMILES.\n",
    "    Returns a DataFrame with one row per molecule.\"\"\"\n",
    "    if desc_names is None:\n",
    "        # Use all standard RDKit descriptors\n",
    "        desc_names = [d[0] for d in Descriptors._descList]\n",
    "\n",
    "    calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "    # Convert SMILES to Mol objects\n",
    "    mols = [safe_mol_from_smiles(s) for s in df_smiles]\n",
    "\n",
    "    rows = []\n",
    "    for m in mols:\n",
    "        if m is None:\n",
    "            # If conversion failed, fill with NaN values\n",
    "            rows.append([np.nan] * len(desc_names))\n",
    "        else:\n",
    "            try:\n",
    "                vals = calc.CalcDescriptors(m)\n",
    "                # Convert descriptor values to floats, fallback to NaN\n",
    "                vals = [float(v) if v is not None else np.nan for v in vals]\n",
    "                rows.append(vals)\n",
    "            except Exception:\n",
    "                rows.append([np.nan] * len(desc_names))\n",
    "\n",
    "    df_desc = pd.DataFrame(rows, columns=desc_names)\n",
    "    df_desc.index = df_smiles.index\n",
    "    return df_desc\n",
    "\n",
    "\n",
    "def mol_to_morgan_fp_array(smiles, radius=2, nBits=1024):\n",
    "    \"\"\"Compute a Morgan fingerprint (as a 0/1 numpy array) from a SMILES string.\"\"\"\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        if m is None:\n",
    "            return np.zeros(nBits, dtype=np.uint8)\n",
    "\n",
    "        bv = AllChem.GetMorganFingerprintAsBitVect(m, radius, nBits)\n",
    "\n",
    "        # Convert RDKit ExplicitBitVect to numpy array\n",
    "        arr = np.zeros((nBits,), dtype=np.uint8)\n",
    "        for i, bit in enumerate(bv):\n",
    "            arr[i] = int(bit)\n",
    "\n",
    "        return arr\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(nBits, dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f83134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:32.672220Z",
     "iopub.status.busy": "2025-12-04T17:02:32.670803Z",
     "iopub.status.idle": "2025-12-04T17:02:32.680405Z",
     "shell.execute_reply": "2025-12-04T17:02:32.678726Z"
    },
    "papermill": {
     "duration": 0.020971,
     "end_time": "2025-12-04T17:02:32.683919",
     "exception": false,
     "start_time": "2025-12-04T17:02:32.662948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_feature_matrix(df, df_desc, fp_bits=1024):\n",
    "    \"\"\"\n",
    "    Build feature matrix by combining RDKit descriptors and Morgan fingerprints.\n",
    "    Returns the full DataFrame and the list of feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Descriptor features\n",
    "    X_desc = df_desc.copy()\n",
    "\n",
    "    # Morgan fingerprints as a stacked array\n",
    "    fps = np.vstack(df['SMILES'].apply(lambda s: mol_to_morgan_fp_array(s, nBits=fp_bits)).values)\n",
    "\n",
    "    # Convert fingerprints to DataFrame\n",
    "    df_fp = pd.DataFrame(fps, columns=[f'FP_{i}' for i in range(fp_bits)], index=df.index)\n",
    "\n",
    "    # Merge descriptors + fingerprints\n",
    "    X_full = pd.concat([X_desc, df_fp], axis=1)\n",
    "\n",
    "    # Ensure numeric output\n",
    "    X_full = X_full.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return X_full, X_full.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb029b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:32.699411Z",
     "iopub.status.busy": "2025-12-04T17:02:32.698899Z",
     "iopub.status.idle": "2025-12-04T17:02:32.714067Z",
     "shell.execute_reply": "2025-12-04T17:02:32.712551Z"
    },
    "papermill": {
     "duration": 0.024965,
     "end_time": "2025-12-04T17:02:32.715930",
     "exception": false,
     "start_time": "2025-12-04T17:02:32.690965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_cleaning(X, max_na_frac=0.3, clip_value=1e8, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Clean feature matrix by removing high-NA columns, imputing missing values,\n",
    "    and clipping extreme outliers.\n",
    "    \"\"\"\n",
    "        \n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Drop columns with too many NaNs\n",
    "    na_frac = X.isnull().mean()\n",
    "    keep_cols = na_frac[na_frac < max_na_frac].index.tolist()\n",
    "    X = X[keep_cols]\n",
    "\n",
    "    # KNN imputer for remaining NaNs\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "    # Clip extreme values\n",
    "    X_imputed = X_imputed.clip(-clip_value, clip_value)\n",
    "\n",
    "    return X_imputed\n",
    "\n",
    "\n",
    "def weighted_mae(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    targets = y_true.columns\n",
    "    K = len(targets)\n",
    "\n",
    "    r = {}\n",
    "    n = {}\n",
    "    for t in targets:\n",
    "        y_valid = y_true[t].dropna()\n",
    "        r[t] = float(y_valid.max() - y_valid.min()) if len(y_valid) > 0 else 1.0\n",
    "        n[t] = max(1, len(y_valid))\n",
    "\n",
    "    sqrt_inv_n = np.array([np.sqrt(1 / n[t]) for t in targets])\n",
    "    weight_norm = K * sqrt_inv_n / np.sum(sqrt_inv_n)\n",
    "\n",
    "    w = {}\n",
    "    for i, t in enumerate(targets):\n",
    "        w[t] = (1.0 / r[t]) * weight_norm[i]\n",
    "\n",
    "    abs_err = 0.0\n",
    "    count = 0\n",
    "    for t in targets:\n",
    "        valid = ~y_true[t].isna()\n",
    "        abs_err += np.sum(w[t] * np.abs(y_pred.loc[valid, t] - y_true.loc[valid, t]))\n",
    "        count += valid.sum()\n",
    "\n",
    "    wmae = abs_err / max(1, count)\n",
    "    return float(wmae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa6ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:32.725274Z",
     "iopub.status.busy": "2025-12-04T17:02:32.724915Z",
     "iopub.status.idle": "2025-12-04T17:02:33.097541Z",
     "shell.execute_reply": "2025-12-04T17:02:33.096307Z"
    },
    "papermill": {
     "duration": 0.380837,
     "end_time": "2025-12-04T17:02:33.100294",
     "exception": false,
     "start_time": "2025-12-04T17:02:32.719457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_selection_pipeline(X, y_for_fs, corr_thresh=0.95, top_k=300):\n",
    "    \"\"\"\n",
    "    Perform feature selection using:\n",
    "    1) Variance filter\n",
    "    2) Correlation filter\n",
    "    3) LightGBM feature importance (keep top_k)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting feature selection...\")\n",
    "    print(\"Initial features:\", X.shape[1])\n",
    "\n",
    "    # 1) Remove zero-variance features\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    X_vt = vt.fit_transform(X)\n",
    "    kept_vt = X.columns[vt.get_support()]\n",
    "    X = pd.DataFrame(X_vt, columns=kept_vt)\n",
    "    print(\"After variance filter:\", X.shape[1])\n",
    "\n",
    "    # 2) Remove highly correlated features\n",
    "    corr = X.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > corr_thresh)]\n",
    "    X = X.drop(columns=to_drop)\n",
    "    print(f\"After correlation filter (th={corr_thresh}): {X.shape[1]}\")\n",
    "\n",
    "    # 3) LightGBM-based feature importance\n",
    "    dtrain = lgb.Dataset(X, label=y_for_fs)\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"max_depth\": -1,\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, dtrain, num_boost_round=150)\n",
    "\n",
    "    # Rank features by importance\n",
    "    importance = model.feature_importance()\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"importance\": importance\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # Keep top-k features\n",
    "    selected = importance_df[\"feature\"].iloc[:top_k].tolist()\n",
    "    X = X[selected]\n",
    "\n",
    "    print(f\"After LGBM importance selection (top {top_k}): {X.shape[1]}\")\n",
    "\n",
    "    return X, selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a644f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:33.109784Z",
     "iopub.status.busy": "2025-12-04T17:02:33.109377Z",
     "iopub.status.idle": "2025-12-04T17:02:33.122736Z",
     "shell.execute_reply": "2025-12-04T17:02:33.121309Z"
    },
    "papermill": {
     "duration": 0.020931,
     "end_time": "2025-12-04T17:02:33.124799",
     "exception": false,
     "start_time": "2025-12-04T17:02:33.103868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_lgb_per_target(X, y, X_test=None, n_splits=5, params=None, num_boost_round=2000, es_rounds=100):\n",
    "    \"\"\"\n",
    "    Train LightGBM using K-fold CV and return OOF predictions, test predictions, and models.\n",
    "    Uses callbacks for early stopping to support different LightGBM builds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default params\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 64,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'seed': 42,\n",
    "            'verbosity': -1\n",
    "        }\n",
    "\n",
    "    # Convert to numpy\n",
    "    X_np = X.values if isinstance(X, pd.DataFrame) else np.asarray(X)\n",
    "    X_test_np = X_test.values if isinstance(X_test, pd.DataFrame) else (np.asarray(X_test) if X_test is not None else None)\n",
    "\n",
    "    # Init result arrays\n",
    "    oof = np.zeros(X_np.shape[0])\n",
    "    preds_test = np.zeros(X_test_np.shape[0]) if X_test_np is not None else None\n",
    "\n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    models = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_np)):\n",
    "        X_tr, X_val = X_np[tr_idx], X_np[val_idx]\n",
    "        y_tr, y_val = np.asarray(y)[tr_idx], np.asarray(y)[val_idx]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "        dval = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "        # Early stopping + silent logging\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(es_rounds, verbose=False),\n",
    "            lgb.log_evaluation(-1)\n",
    "        ]\n",
    "\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dval],\n",
    "                          num_boost_round=num_boost_round,\n",
    "                          callbacks=callbacks)\n",
    "\n",
    "        # Store predictions\n",
    "        oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        if X_test_np is not None:\n",
    "            preds_test += model.predict(X_test_np, num_iteration=model.best_iteration) / n_splits\n",
    "\n",
    "        models.append(model)\n",
    "        print(f\"  Fold {fold} done. best_iter={model.best_iteration}\")\n",
    "\n",
    "    return oof, preds_test, models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40b23a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:02:33.136519Z",
     "iopub.status.busy": "2025-12-04T17:02:33.136154Z",
     "iopub.status.idle": "2025-12-04T17:06:12.524228Z",
     "shell.execute_reply": "2025-12-04T17:06:12.523220Z"
    },
    "papermill": {
     "duration": 219.395423,
     "end_time": "2025-12-04T17:06:12.526255",
     "exception": false,
     "start_time": "2025-12-04T17:02:33.130832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing RDKit descriptors for train...\n",
      "Computing RDKit descriptors for test...\n",
      "Building feature matrices...\n",
      "\n",
      "Running global feature selection using Tg...\n",
      "Starting feature selection...\n",
      "Initial features: 1229\n",
      "After variance filter: 1148\n",
      "After correlation filter (th=0.93): 1079\n",
      "After LGBM importance selection (top 300): 300\n",
      "\n",
      "========================================\n",
      "Training target: Tg\n",
      "  Fold 0 done. best_iter=6\n",
      "  Fold 1 done. best_iter=1\n",
      "  Fold 2 done. best_iter=1\n",
      "  Fold 3 done. best_iter=5\n",
      "  Fold 4 done. best_iter=11\n",
      "OOF MAE for Tg: 88.069889\n",
      "\n",
      "========================================\n",
      "Training target: FFV\n",
      "  Fold 0 done. best_iter=1\n",
      "  Fold 1 done. best_iter=1\n",
      "  Fold 2 done. best_iter=2\n",
      "  Fold 3 done. best_iter=2\n",
      "  Fold 4 done. best_iter=2\n",
      "OOF MAE for FFV: 0.020938\n",
      "\n",
      "========================================\n",
      "Training target: Tc\n",
      "  Fold 0 done. best_iter=3\n",
      "  Fold 1 done. best_iter=8\n",
      "  Fold 2 done. best_iter=2\n",
      "  Fold 3 done. best_iter=9\n",
      "  Fold 4 done. best_iter=3\n",
      "OOF MAE for Tc: 0.076767\n",
      "\n",
      "========================================\n",
      "Training target: Density\n",
      "  Fold 0 done. best_iter=27\n",
      "  Fold 1 done. best_iter=74\n",
      "  Fold 2 done. best_iter=1\n",
      "  Fold 3 done. best_iter=4\n",
      "  Fold 4 done. best_iter=30\n",
      "OOF MAE for Density: 0.108246\n",
      "\n",
      "========================================\n",
      "Training target: Rg\n",
      "  Fold 0 done. best_iter=22\n",
      "  Fold 1 done. best_iter=8\n",
      "  Fold 2 done. best_iter=20\n",
      "  Fold 3 done. best_iter=57\n",
      "  Fold 4 done. best_iter=6\n",
      "OOF MAE for Rg: 3.904281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOUT_PATH = \"/kaggle/working/submission.csv\"\\npredictions.to_csv(OUT_PATH, index=False)\\nprint(\"\\nSaved submission to:\", OUT_PATH)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset indices for consistency\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# Compute RDKit descriptors\n",
    "print(\"Computing RDKit descriptors for train...\")\n",
    "train_desc = compute_rdkit_descriptors(train[\"SMILES\"])\n",
    "print(\"Computing RDKit descriptors for test...\")\n",
    "test_desc = compute_rdkit_descriptors(test[\"SMILES\"])\n",
    "\n",
    "# Build feature matrices (descriptors + fingerprints)\n",
    "print(\"Building feature matrices...\")\n",
    "FP_BITS = 1024\n",
    "X_train_full, feat_names = build_feature_matrix(train, train_desc, fp_bits=FP_BITS)\n",
    "X_test_full, _ = build_feature_matrix(test, test_desc, fp_bits=FP_BITS)\n",
    "\n",
    "# Clean and align features\n",
    "X_train_full = data_cleaning(X_train_full)\n",
    "X_test_full = X_test_full.reindex(columns=X_train_full.columns, fill_value=np.nan)\n",
    "X_test_full = X_test_full.fillna(X_test_full.median())\n",
    "\n",
    "# Global feature selection using Tg (if available)\n",
    "if \"Tg\" in train.columns:\n",
    "    df_fs = train.dropna(subset=[\"Tg\"]).reset_index(drop=True)\n",
    "    X_fs = X_train_full.loc[df_fs.index]\n",
    "    y_fs = df_fs[\"Tg\"].values\n",
    "\n",
    "    print(\"\\nRunning global feature selection using Tg...\")\n",
    "    X_fs_new, selected_features = feature_selection_pipeline(X_fs, y_fs,\n",
    "                                                             corr_thresh=0.93, top_k=300)\n",
    "\n",
    "    # Apply selected features\n",
    "    X_train_full = X_train_full[selected_features]\n",
    "    X_test_full = X_test_full[selected_features]\n",
    "else:\n",
    "    print(\"Tg not found — skipping feature selection.\")\n",
    "\n",
    "# Prediction output container\n",
    "predictions = pd.DataFrame({\"id\": test[\"id\"]})\n",
    "models_in_memory = {}\n",
    "results = []\n",
    "\n",
    "# Train a separate model per target\n",
    "for target in targets:\n",
    "    print(\"\\n========================================\")\n",
    "    print(f\"Training target: {target}\")\n",
    "\n",
    "    if target not in train.columns:\n",
    "        continue\n",
    "\n",
    "    df_t = train.dropna(subset=[target]).reset_index(drop=True)\n",
    "    if len(df_t) < 30:  # skip if too few samples\n",
    "        print(\"Too few samples — skipped.\")\n",
    "        continue\n",
    "\n",
    "    # Extract aligned training data\n",
    "    idx = df_t.index\n",
    "    X_t = X_train_full.loc[idx].reset_index(drop=True)\n",
    "    y_t = df_t[target].values\n",
    "\n",
    "    # Standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X_t), columns=X_t.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_full), columns=X_test_full.columns)\n",
    "\n",
    "    # LightGBM training\n",
    "    oof, test_preds, model_list = train_lgb_per_target(X_scaled, y_t, X_test_scaled)\n",
    "\n",
    "    # Store predictions and model objects\n",
    "    predictions[target] = test_preds\n",
    "    models_in_memory[target] = {\n",
    "        \"models\": model_list,\n",
    "        \"scaler\": scaler,\n",
    "        \"features\": X_scaled.columns.tolist(),\n",
    "        \"oof\": oof,\n",
    "    }\n",
    "\n",
    "    # Compute OOF MAE\n",
    "    mae = np.mean(np.abs(oof - y_t))\n",
    "    results.append(mae)\n",
    "    print(f\"OOF MAE for {target}: {mae:.6f}\")\n",
    "\n",
    "# Fill targets missing from predictions\n",
    "for t in targets:\n",
    "    if t not in predictions.columns:\n",
    "        predictions[t] = train[t].median()\n",
    "\n",
    "# Save submission file\n",
    "'''\n",
    "OUT_PATH = \"/kaggle/working/submission.csv\"\n",
    "predictions.to_csv(OUT_PATH, index=False)\n",
    "print(\"\\nSaved submission to:\", OUT_PATH)\n",
    "'''\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "sourceId": 74608,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 241.88678,
   "end_time": "2025-12-04T17:06:15.163097",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T17:02:13.276317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
